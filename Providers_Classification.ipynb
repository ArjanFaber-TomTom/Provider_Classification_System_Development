{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43900d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm \n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c0ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_retrieval(threshold = 0.65):\n",
    "    data_path = \"./Data_Providers/\"\n",
    "    df_providers_class_content = pd.read_csv(data_path + 'content_overview_new_202509.csv')\n",
    "    df_providers_class_service = pd.read_csv(data_path + 'service_overview_new_202509.csv')\n",
    "    df_providers_class_content = df_providers_class_content[df_providers_class_content.columns[0:-1]].dropna()\n",
    "    df_providers_class_service = df_providers_class_service[df_providers_class_service.columns[0:-1]].dropna()\n",
    "    print(df_providers_class_content.describe()), print(df_providers_class_service.describe())\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    row = []\n",
    "\n",
    "    for i in range(len(df_providers_class_service)):\n",
    "        for j in range(len(df_providers_class_content)):\n",
    "            if df_providers_class_content[\"instance\"][j][0:-1] in df_providers_class_service.loc[i, \"server\"] and df_providers_class_content[\"feed\"][j] == df_providers_class_service[\"feed\"][i] and df_providers_class_content[\"date\"][j] == df_providers_class_service[\"date\"][i]:\n",
    "        \n",
    "                # build row\n",
    "                row = {\n",
    "                \"feed\": df_providers_class_service.loc[i, \"feed\"],\n",
    "                \"server\": df_providers_class_service.loc[i, \"server\"],\n",
    "                \"map_matching\": df_providers_class_content.loc[j, \"map_matching\"],\n",
    "                \"exclusive\": df_providers_class_content.loc[j, \"exclusive\"],\n",
    "                \"redundant\": df_providers_class_content.loc[j, \"redundant\"],\n",
    "                \"accidents\": df_providers_class_service.loc[i, \"accidents\"],\n",
    "                \"hazards\": df_providers_class_service.loc[i, \"hazards\"],\n",
    "                \"closures\": df_providers_class_service.loc[i, \"closures\"],\n",
    "                \"freshness\": df_providers_class_service.loc[i, \"freshness\"],\n",
    "                \"contribution\": df_providers_class_service.loc[i, \"contribution\"],\n",
    "                \"usability\": df_providers_class_service.loc[i, \"usability\"],\n",
    "                \"availability\": df_providers_class_service.loc[i, \"availability\"],\n",
    "\n",
    "                }\n",
    "                row = pd.DataFrame([row])\n",
    "                # append row to dataframe\n",
    "                df_new = pd.concat([df_new, row])\n",
    "          \n",
    "        print(\"cycle i:\", i, \" out of \", len(df_providers_class_service))\n",
    "\n",
    "    merged_df = df_new\n",
    "    df_complete = merged_df\n",
    "    df_complete['freshness'] = (df_complete['freshness'] - df_complete['freshness'].min()) / (df_complete['freshness'].max() - df_complete['freshness'].min())\n",
    "    df = df_complete[df_complete.columns[2:]]\n",
    "    corr_matrix = df.corr()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.savefig(\"corr_matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # Extract attributes with absolute correlation above 0.65 (excluding self-correlation)\n",
    "    high_corr_cols = set()\n",
    "    \n",
    "    for col in corr_matrix.columns:\n",
    "        for idx in corr_matrix.index:\n",
    "            if col != idx and abs(corr_matrix.loc[idx, col]) > threshold:\n",
    "                high_corr_cols.add(col)\n",
    "                high_corr_cols.add(idx)\n",
    "\n",
    "    # Convert to list\n",
    "    high_corr_cols = list(high_corr_cols)\n",
    "    print(\"Attributes with absolute correlation above 0.65:\", high_corr_cols)\n",
    "\n",
    "    # Optional: store these attributes in a new dataframe\n",
    "    df_high_corr = df[high_corr_cols]\n",
    "\n",
    "    df_subset = df\n",
    "\n",
    "    # Plot the average value for each column\n",
    "    df_subset.mean().plot(kind='bar', figsize=(12, 6))\n",
    "    plt.ylabel('Average value')\n",
    "    plt.title('Average Scores per Attribute')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"avg_attributes.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return df_subset, df_high_corr, df_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def cluster_gmm(df, a=1,b=1.960, grades = ['A+', 'A', 'B', 'C', 'D']):\n",
    "\n",
    "    df = df[['map_matching', 'exclusive', 'redundant', 'accidents',\n",
    "       'hazards', 'closures', 'freshness', 'contribution', 'usability',\n",
    "       'availability']]\n",
    "\n",
    "    # --- Fit GMM ---\n",
    "    gmm = GaussianMixture(n_components=len(grades), covariance_type='full', random_state=42)\n",
    "    df['cluster_GMM'] = gmm.fit_predict(df)\n",
    "\n",
    "    # --- Probabilities for each cluster ---\n",
    "    proba = gmm.predict_proba(df.drop(columns=['cluster_GMM']))\n",
    "    df[[f'cluster_{i}_prob' for i in range(5)]] = proba\n",
    "\n",
    "    # --- Cluster summary\n",
    "    cluster_summary = df.groupby('cluster_GMM')[['map_matching', 'redundant', 'freshness','exclusive',\n",
    "                                         'availability', 'contribution', 'usability']].agg(['mean', 'min', 'max', 'count'])\n",
    "\n",
    "    mean_cols = cluster_summary.loc[:, pd.IndexSlice[:, 'mean']]\n",
    "    mean_cols.columns = mean_cols.columns.droplevel(1)\n",
    "\n",
    "    # Average mean score\n",
    "    cluster_summary['average_mean'] = mean_cols.mean(axis=1)\n",
    "\n",
    "    # Variance score\n",
    "    cluster_variance = df.groupby('cluster_GMM')[['map_matching', 'redundant', 'exclusive',\n",
    "                                          'availability', 'freshness', 'contribution', 'usability']].var()\n",
    "    cluster_summary['variance_score'] = cluster_variance.mean(axis=1)\n",
    "\n",
    "    # --- Extract means for custom scoring ---\n",
    "    map_matching_mean = cluster_summary[('map_matching', 'mean')]\n",
    "    exclusive_mean = cluster_summary[('exclusive', 'mean')]\n",
    "    redundant_mean = cluster_summary[('redundant', 'mean')]\n",
    "    availability_mean = cluster_summary[('availability', 'mean')]\n",
    "    freshness_mean = cluster_summary[('freshness', 'mean')]\n",
    "    contribution_mean = cluster_summary[('contribution', 'mean')]\n",
    "    usability_mean = cluster_summary[('usability', 'mean')]\n",
    "\n",
    "    # --- Custom score formula ---\n",
    "    cluster_summary['custom_score'] = (\n",
    "        map_matching_mean + exclusive_mean + availability_mean +\n",
    "        freshness_mean + contribution_mean + usability_mean - redundant_mean    \n",
    "    )\n",
    "\n",
    "    # Rank clusters by custom score\n",
    "    cluster_summary['custom_rank'] = cluster_summary['custom_score'].rank(ascending=False)\n",
    "    cluster_summary_mean = cluster_summary.sort_values('custom_rank', ascending=True)\n",
    "\n",
    "    # Assign grades (custom score)\n",
    "    \n",
    "    cluster_summary_mean['grade'] = pd.Series(grades[:len(cluster_summary_mean)],\n",
    "                                          index=cluster_summary_mean.index)\n",
    "\n",
    "    # --- Rank by variability ---\n",
    "    cluster_summary_var = cluster_summary.sort_values('variance_score', ascending=True)\n",
    "    cluster_summary_var['grade'] = pd.Series(grades[:len(cluster_summary_var)],\n",
    "                                         index=cluster_summary_var.index)\n",
    "\n",
    "    rows_lb = []\n",
    "    rows_ub = []\n",
    "    for i in range(len(grades)):\n",
    "      rows_lb.append(a * cluster_summary_mean.custom_score[i] - b * np.sqrt(cluster_summary_var.variance_score[i]) )\n",
    "    cluster_summary['threshold_lb'] = rows_lb\n",
    "\n",
    "    for i in range(len(grades)):\n",
    "      rows_ub.append(a * cluster_summary_mean.custom_score[i] + b * np.sqrt(cluster_summary_var.variance_score[i]) )\n",
    "    cluster_summary['threshold_ub'] = rows_ub\n",
    "\n",
    "    # Rank by threshold\n",
    "    cluster_summary_thld = cluster_summary.sort_values('threshold_ub', ascending=False)\n",
    "    cluster_summary_thld['grade'] = pd.Series(grades[:len(cluster_summary_thld)],\n",
    "                                          index=cluster_summary_thld.index)\n",
    "\n",
    "    score_pca = silhouette_score(df, df['cluster_GMM'])\n",
    "    bic = gmm.bic(df[['map_matching', 'exclusive', 'redundant', 'accidents',\n",
    "       'hazards', 'closures', 'freshness', 'contribution', 'usability',\n",
    "       'availability']])\n",
    "    aic = gmm.aic(df[['map_matching', 'exclusive', 'redundant', 'accidents',\n",
    "       'hazards', 'closures', 'freshness', 'contribution', 'usability',\n",
    "       'availability']])\n",
    "    \n",
    "    return df, cluster_summary_mean[['custom_score', 'grade']], cluster_summary_var[['variance_score', 'grade']], cluster_summary_thld[['threshold_lb','threshold_ub', 'grade']].sort_index(), score_pca, bic,aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec15eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmeans(df, a=1,b=1.960,  grades = ['A+', 'A', 'B', 'C', 'D'] ):\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "    df['cluster'] = kmeans.fit_predict(df)\n",
    "\n",
    "    cluster_summary = df.groupby('cluster')[['map_matching', 'redundant', 'freshness','exclusive', 'availability', 'contribution', 'usability']].agg(['mean', 'min', 'max', 'count'])\n",
    "    mean_cols = cluster_summary.loc[:, pd.IndexSlice[:, 'mean']]\n",
    "\n",
    "    # Rename columns for easier access (optional)\n",
    "    mean_cols.columns = mean_cols.columns.droplevel(1)  # drop second level 'mean'\n",
    "\n",
    "    # Now calculate average of the three feature means for each cluster\n",
    "    cluster_summary['average_mean'] = mean_cols.mean(axis=1)\n",
    "\n",
    "    cluster_variance = df.groupby('cluster')[['map_matching', 'redundant', 'exclusive', 'availability', 'freshness', 'contribution', 'usability']].var()\n",
    "    cluster_summary['variance_score'] = cluster_variance.mean(axis=1)\n",
    "\n",
    "    # Extract individual mean values\n",
    "    map_matching_mean = cluster_summary[('map_matching', 'mean')]\n",
    "    exclusive_mean = cluster_summary[('exclusive', 'mean')]\n",
    "    redundant_mean = cluster_summary[('redundant', 'mean')]\n",
    "    availability_mean = cluster_summary[('availability', 'mean')]\n",
    "    freshness_mean = cluster_summary[('freshness', 'mean')]\n",
    "    contribution_mean = cluster_summary[('contribution', 'mean')]\n",
    "    usability_mean = cluster_summary[('usability', 'mean')]\n",
    "    # Apply the custom scoring formula\n",
    "    cluster_summary['custom_score'] = map_matching_mean + exclusive_mean + availability_mean + freshness_mean + contribution_mean + usability_mean - redundant_mean\n",
    "\n",
    "    # Rank clusters by custom score\n",
    "    cluster_summary['custom_rank'] = cluster_summary['custom_score'].rank(ascending=False)\n",
    "\n",
    "    # Sort clusters by average_mean descending (best quality first)\n",
    "    cluster_summary_mean = cluster_summary.sort_values('custom_rank', ascending=True)\n",
    "\n",
    "    # Define grades to assign by rank\n",
    " \n",
    "\n",
    "    # Assign grades based on rank position\n",
    "    cluster_summary_mean['grade'] = pd.Series(grades[:len(cluster_summary_mean)], index=cluster_summary_mean.index)\n",
    "    cluster_summary_mean[['custom_score', 'grade']]\n",
    "\n",
    "    # Sort clusters by variability ascending (best quality first)\n",
    "    cluster_summary_var = cluster_summary.sort_values('variance_score', ascending=True)\n",
    "\n",
    "    # Define grades to assign by rank\n",
    "  \n",
    "\n",
    "    # Assign grades based on rank position\n",
    "    cluster_summary_var['grade'] = pd.Series(grades[:len(cluster_summary_var)], index=cluster_summary_var.index)\n",
    "    cluster_summary_var[['variance_score', 'grade']]\n",
    "    cluster_summary['threshold'] = np.zeros(5)\n",
    "    rows_lb = []\n",
    "    rows_ub = []\n",
    "    for i in range(5):\n",
    "        rows_lb.append(a * cluster_summary_mean.custom_score[i] - b * np.sqrt(cluster_summary_var.variance_score[i]) )\n",
    "    cluster_summary['threshold_lb'] = rows_lb\n",
    "\n",
    "    for i in range(5):\n",
    "        rows_ub.append(a * cluster_summary_mean.custom_score[i] + b * np.sqrt(cluster_summary_var.variance_score[i]) )\n",
    "    cluster_summary['threshold_ub'] = rows_ub\n",
    "\n",
    "    # Sort clusters by variability ascending (best quality first)\n",
    "    cluster_summary_thld= cluster_summary.sort_values('threshold_ub', ascending=False)\n",
    "  \n",
    "\n",
    "    # Assign grades based on rank position\n",
    "    cluster_summary_thld['grade'] = pd.Series(grades[:len(cluster_summary_thld)], index=cluster_summary_thld.index)\n",
    "\n",
    "    score_pca = silhouette_score(df, df['cluster'])\n",
    "    \n",
    "    return df, cluster_summary_mean[['custom_score', 'grade']], cluster_summary_var[['variance_score', 'grade']], cluster_summary_thld[['threshold_lb', 'threshold_ub','grade']].sort_index(), score_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f371cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Needed even if not directly used\n",
    "\n",
    "def plot(df, var_name):\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 6))  # Wider layout for side-by-side\n",
    "\n",
    "    # Define your triplets and titles\n",
    "    plot_configs = [\n",
    "    ('accidents', 'hazards', 'closures', 'Incident Coverage'),\n",
    "    ('exclusive', 'redundant', 'contribution', 'Data Richness'),\n",
    "    ('freshness', 'availability', 'map_matching', 'Service Performance')]\n",
    "\n",
    "    # Loop to create subplots\n",
    "    for idx, (x_col, y_col, z_col, title) in enumerate(plot_configs, start=1):\n",
    "        ax = fig.add_subplot(1, 3, idx, projection='3d')\n",
    "    \n",
    "        scatter = ax.scatter(\n",
    "            df[x_col],\n",
    "            df[y_col],\n",
    "            df[z_col],\n",
    "            c=df[var_name],\n",
    "            cmap='tab10',\n",
    "            s=50,\n",
    "            alpha=0.8\n",
    "        )\n",
    "    \n",
    "        ax.set_xlabel(x_col)\n",
    "        ax.set_ylabel(y_col)\n",
    "        ax.set_zlabel(z_col)\n",
    "\n",
    "        # Apply consistent inversion logic if needed (edit per axis logic)\n",
    "        ax.invert_xaxis()\n",
    "        ax.invert_yaxis()\n",
    "        ax.invert_zaxis()\n",
    "    \n",
    "        ax.set_title(title)\n",
    "\n",
    "    # Shared legend (outside of plot area)\n",
    "    handles = [\n",
    "        Line2D([0], [0], marker='o', color='w', label=f'Cluster {i}',\n",
    "           markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=8)\n",
    "        for i in sorted(df[var_name].unique())\n",
    "    ]\n",
    "\n",
    "    fig.legend(handles=handles, title='Clusters', loc='center right', bbox_to_anchor=(1.05, 0.5))\n",
    "    fig.suptitle('3D Scatter Plots by Dimension Group', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 1])  # shrink plot area slightly more\n",
    "    fig.subplots_adjust(wspace=0.3)        # increase horizontal space between subplots\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2cb426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_requirements():\n",
    "\n",
    "    market_req = pd.DataFrame({\"Accuracy_lb\": [0, 0.15, 0.5,0.85,0.95],\"Completeness_lb\": [0,0.10,0.4,0.6,0.90],\"Correctness_lb\": [0,0.15,0.5,0.85,0.95], \"Availability_lb\": [0,0.85,0.95,0.97,0.99], \"Timelineness_lb\": [0,0.15,0.8,0.85,0.95], \"Usability_lb\": [0,0.15,0.5,0.85,0.95] , \n",
    "                              \"Accuracy_ub\": [0.15, 0.50, 0.85,0.95,1],\"Completeness_ub\": [0.1,0.4,0.6,0.9,1],\"Correctness_ub\": [0.15,0.5,0.85,0.95,1], \"Availability_ub\": [0.85,0.95,0.97,0.99,1], \"Timelineness_ub\": [0.15,0.8,0.85,0.95,1], \"Usability_ub\": [0.15,0.5,0.85,0.95,1] })\n",
    "    market_req['Classification'] = [\"D\", \"C\", \"B\", \"A\", \"A+\"]\n",
    "    \n",
    "    return market_req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, corr, df_describe = data_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cde1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clus, df_clus_mean, df_clus_var, df_clus_thld, score_gmm, bic, aic =  cluster_gmm(df)\n",
    "df_kmean, df_kmean_mean,df_kmean_var, df_kmeans_thld, kmean_metric = cluster_kmeans(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f21d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if score_gmm > kmean_metric:\n",
    "    var_name = 'cluster_GMM'\n",
    "    plot(df_clus, var_name)\n",
    "    print (\"Silhouette score: \" , score_gmm)\n",
    "\n",
    "else:\n",
    "    var_name = 'cluster'\n",
    "    plot(df, var_name)\n",
    "    print (\"Silhouette score: \" , kmean_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004455cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "market_req = market_requirements()\n",
    "market_req[market_req.columns[0]][market_req['Classification']=='A'] , market_req[market_req.columns[6]][market_req['Classification']=='A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ddfb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clus_thld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18248ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans_thld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05213431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
